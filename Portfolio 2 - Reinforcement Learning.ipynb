{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f9bed7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #a3e4d7 ; text-align:center; vertical-align: middle; padding:40px 0; margin-top:30px\">\n",
    "<h1 style=\"color:black\"> Portfolio 2 - Reinforcement Learning</h1>\n",
    "<b style=\"color:white\"> Jort Akershoek, Julia Boschman, Daan Eising, Rogier Gernaat</b>\n",
    "    </div>\n",
    "\n",
    "<a name='start'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243e3e37",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [H1: Inleiding](#1.0)\n",
    "    - [&sect;1.1:Uileg inhoud Notebook ](#1.1)\n",
    "    - [&sect;1.2: Imports](#1.2)\n",
    "2. [H2: EDA](#2.0)\n",
    "    - [&sect;2.1: Probleemanalyse](#2.1)\n",
    "    - [&sect;2.2: Doelstelling](#2.2)\n",
    "3. [H3: EDA](#3.0)\n",
    "    - [&sect;3.1: Het model](#3.1)\n",
    "4. [H4: EDA](#4.0)\n",
    "    - [&sect;4.1: Vergelijking met baseline](#4.1)\n",
    "    - [&sect;4.2: Resultaten](#4.2)\n",
    "5. [H5: EDA](#5.0)\n",
    "    - [&sect;5.1: Probleemstelling](#5.1)\n",
    "    - [&sect;5.2: Methodologie](#5.2)\n",
    "    - [&sect;5.3: Resultaten](#5.3)\n",
    "    - [&sect;5.4: Reflectie](#5.4)\n",
    "6. [H6: Literatuurlijst](#6.0)\n",
    "7. [Beoordeling](#Beoordeling.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a2b24",
   "metadata": {},
   "source": [
    "[Back to the top](#start)\n",
    "<a name='1.0'></a>\n",
    "\n",
    "<div style=\"background-color:#a3e4d7; text-align:center; vertical-align:middle; padding:10px 0; margin-top:5px; margin-bottom:5px\">\n",
    "        <h2 style=\"color:white\"><strong>H1: Inleiding</strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3f1f41",
   "metadata": {},
   "source": [
    "<a name='1.1'></a>\n",
    "<h3>&sect;1.1: Uitleg inhoud Notebook</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253f2476",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c54a5f7",
   "metadata": {},
   "source": [
    "<a name='1.2'></a>\n",
    "<h3>&sect;1.2: Imports</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f6dd8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae577220",
   "metadata": {},
   "source": [
    "[Back to the top](#start)\n",
    "<a name='2.0'></a>\n",
    "\n",
    "<div style=\"background-color:#a3e4d7; text-align:center; vertical-align:middle; padding:10px 0; margin-top:5px; margin-bottom:5px\">\n",
    "        <h2 style=\"color:white\"><strong>H2: Kies een use case</strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4d22a9",
   "metadata": {},
   "source": [
    "- [ ] Kies een omgeving van www.pettingzoo.com of Gymnasium Documentation (niet een omgeving die al eerder gebruikt is door jullie of in de les is behandeld.\n",
    "- [ ] Analyseer het probleem: Waarom is juist RL geschikt om deze uitdaging aan te pakken? Hoe onderscheidt RL zich hier van andere methoden, zoals supervised learning of niet AI-oplossingen?\n",
    "- [ ] Resultaat: Een helder uitgewerkte Probleemdefinitie en Doelstelling\n",
    "    - [ ] Probleemdefinitie \n",
    "    - [ ] Doelstelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724e6c8",
   "metadata": {},
   "source": [
    "<a name='2.1'></a>\n",
    "<h3>&sect;2.1: Probleemanalyse</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d529e946",
   "metadata": {},
   "source": [
    "Reinforcement Learning (RL) heeft de afgelopen jaren veel vooruitgang laten zien bij het oplossen van problemen waarbij een computer (agent) moet leren door zelf dingen uit te proberen in een omgeving. Traditionele methodes, zoals supervised learning, zijn meestal minder geschikt voor situaties waarin de agent continu moet reageren op directe feedback en snel veranderende omstandigheden. Dit is vooral merkbaar in real-time spellen zoals VizDoom. In VizDoom moet een agent zelfstandig beslissingen nemen, reageren op onverwachte situaties, vijanden ontwijken of verslaan, en duidelijke doelen behalen in een dynamische spelomgeving.\n",
    "\n",
    "Reinforcement Learning is bijzonder geschikt voor dynamische omgevingen zoals VizDoom omdat de agent actief leert door interactie met de omgeving en directe feedback krijgt via beloningen (rewards). Dit stelt de agent in staat zich continu aan te passen aan veranderingen in het spel. De voordelen van RL ten opzichte van supervised learning en niet-AI methoden in VizDoom zijn duidelijk. Supervised learning vereist vooraf gelabelde data en kan zich moeilijk aanpassen aan onverwachte situaties, terwijl RL juist leert van eigen ervaring en zich flexibel aanpast. Niet-AI methoden zijn vaak gebaseerd op vaste regels en zijn niet in staat om zelfstandig te leren of zich snel aan te passen aan nieuwe situaties, wat juist essentieel is in dynamische spellen zoals VizDoom.\n",
    "\n",
    "Daarom willen we onderzoeken hoe een agent via RL effectief kan leren handelen in VizDoom.\n",
    "\n",
    "#### Centrale probleemstelling:\n",
    "- \"Hoe kan een Reinforcement Learning-agent effectief leren opereren en goede resultaten behalen in de dynamische en uitdagende spelomgeving van VizDoom?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c681724",
   "metadata": {},
   "source": [
    "<a name='2.2'></a>\n",
    "<h3>&sect;2.2: Doelstelling</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389d223e",
   "metadata": {},
   "source": [
    "Het doel van deze opdracht is het ontwikkelen, implementeren en valideren van een Reinforcement Learning-agent die binnen de VizDoom-omgeving zelfstandig leert om optimale beslissingen te nemen en specifieke taken succesvol uit te voeren. De volgende subdoelstellingen zijn hierbij geformuleerd:\n",
    "\n",
    "1. Onderzoek en analyse:\n",
    "    - Bepaal waarom Reinforcement Learning bijzonder geschikt is voor het oplossen van problemen in dynamische omgevingen zoals VizDoom.\n",
    "    - Analyseer de voordelen van RL ten opzichte van supervised learning en niet-AI methoden specifiek in de context van VizDoom.\n",
    "\n",
    "2. Ontwerp en implementatie:\n",
    "    - Ontwerp en implementeer een baseline-strategie (bijvoorbeeld random of een eenvoudige heuristiek) om prestaties te kunnen benchmarken.\n",
    "    - Ontwikkel zelfstandig een RL-algoritme (zoals Q-learning of Sarsa) met minimale afhankelijkheid van bestaande RL-packages.\n",
    "    - Optimaliseer het geïmplementeerde RL-algoritme door systematisch te experimenteren met verschillende hyperparameters (bijvoorbeeld exploration-exploitation balans).\n",
    "\n",
    "3. Evaluatie en validatie:\n",
    "    - Vergelijk de prestaties van het ontwikkelde RL-model met de baseline-strategie door middel van heldere en reproduceerbare experimenten.\n",
    "    - Analyseer en visualiseer gedetailleerd de prestaties van het RL-model met behulp van reward-curves en andere relevante prestatie-indicatoren.\n",
    "\n",
    "Door deze doelstellingen uit te voeren, krijgen we beter inzicht in hoe Reinforcement Learning gebruikt kan worden in complexe en dynamische spelomgevingen zoals VizDoom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1279794",
   "metadata": {},
   "source": [
    "[Back to the top](#start)\n",
    "<a name='2.0'></a>\n",
    "\n",
    "<div style=\"background-color:#a3e4d7; text-align:center; vertical-align:middle; padding:10px 0; margin-top:5px; margin-bottom:5px\">\n",
    "        <h2 style=\"color:white\"><strong>H3: Ontwerp en Implementatie van het RL-model</strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cfc656",
   "metadata": {},
   "source": [
    "- [ ] Ontwikkel een baseline strategie zoals bijvoorbeeld een random policy of een andere simpele heuristiek\n",
    "- Implementeer zelf een RL- algoritme (bv Q-Learning, Sarsa). *Implementeer zo veel modelijk van het RL-algoritme zelf zonder gebruik te maken van packages waarin de algoritmes voor je gebouwd zijn.*\n",
    "- [ ] Experimenteer met hyperparameters (zoals bijvoorbeeld de exploration-exploitation parameter). Documenteer dit ook.\n",
    "- [ ] Zorg dat je systeem reproduceerbaar is met duidelijke documentatie, inclusief een gestructureerde mappenstructuur, een README en een requirements.txt.\n",
    "    - [ ] Duidelijke documentatie\n",
    "    - [ ] Gestructureerde mappenstructuur\n",
    "    - [ ] README\n",
    "    - [ ] requirements.txt\n",
    "- [ ] Train je model en documenteer belangrijke inzichten over het gedrag en de presentatie van het systeem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07eb274",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1e58657",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "<h3>&sect;3.1: Het model</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94486fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b149920",
   "metadata": {},
   "source": [
    "[Back to the top](#start)\n",
    "<a name='4.0'></a>\n",
    "\n",
    "<div style=\"background-color:#a3e4d7; text-align:center; vertical-align:middle; padding:10px 0; margin-top:5px; margin-bottom:5px\">\n",
    "        <h2 style=\"color:white\"><strong>H4: Valideer en vergelijk</strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e0acc",
   "metadata": {},
   "source": [
    "- [ ] Vergelijk je RL-model met ten minste één baseline.\n",
    "- [ ] Analyseer en visualiseer je resultaten: Hoe presteert je RL-systeem? Wat laten de Reward-curves of andere indicatoren zien? Waar blinkt het model uit, en waar liggen de beperkingen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127bc592",
   "metadata": {},
   "source": [
    "<a name='3.1'></a>\n",
    "<h3>&sect;3.1: Vergelijking met baseline</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b69dc7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "994ee213",
   "metadata": {},
   "source": [
    "<a name='3.2'></a>\n",
    "<h3>&sect;3.2: Resultaten</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aec4a76",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53d34b9e",
   "metadata": {},
   "source": [
    "[Back to the top](#start)\n",
    "<a name='5.0'></a>\n",
    "\n",
    "<div style=\"background-color:#a3e4d7; text-align:center; vertical-align:middle; padding:10px 0; margin-top:5px; margin-bottom:5px\">\n",
    "        <h2 style=\"color:white\"><strong>H5: Beschrijving van het werk</strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251b236",
   "metadata": {},
   "source": [
    "- [ ] Schrijf een academisch rapport waarin je probleemstelling, methodologie, resultaten en reflectie helder uiteenzet. Zorg dat de structuur wetenschappelijk onderbouwd is en verwijst naar minimaal vijf relevante bronnen.\n",
    "    - [ ] Probleemstelling\n",
    "    - [ ] Methodologie\n",
    "    - [ ] Resultaten\n",
    "    - [ ] Reflectie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1caaee1f",
   "metadata": {},
   "source": [
    "<a name='5.1'></a>\n",
    "<h3>&sect;5.1: Probleemstelling</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04972f8b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b89b4d90",
   "metadata": {},
   "source": [
    "<a name='5.2'></a>\n",
    "<h3>&sect;5.2: Methodologie</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222ac3e6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8d4e62c",
   "metadata": {},
   "source": [
    "<a name='5.3'></a>\n",
    "<h3>&sect;5.3: Resultaten</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e60146",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ff48581",
   "metadata": {},
   "source": [
    "<a name='5.4'></a>\n",
    "<h3>&sect;5.4: Reflectie</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d178d642",
   "metadata": {},
   "source": [
    "[Back to the top](#start)\n",
    "<a name='6.0'></a>\n",
    "\n",
    "<div style=\"background-color:#a3e4d7; text-align:center; vertical-align:middle; padding:10px 0; margin-top:5px; margin-bottom:5px\">\n",
    "        <h2 style=\"color:white\"><strong>H6: Literatuurlijst</strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3427592",
   "metadata": {},
   "source": [
    "##### Bronnen voor Probleemstelling en Doelstelling:\n",
    "- [Probleemstelling](https://www.scribbr.nl/starten-met-je-scriptie/probleemstelling/)\n",
    "- [Doelstelling](https://www.scribbr.nl/starten-met-je-scriptie/doelstelling/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5eb2a4",
   "metadata": {},
   "source": [
    "[Back to the top](#start)\n",
    "<a name='Beoordeling'></a>\n",
    "\n",
    "<div style=\"background-color:#a3e4d7; text-align:center; vertical-align:middle; padding:10px 0; margin-top:5px; margin-bottom:5px\">\n",
    "        <h2 style=\"color:white\"><strong>Beoordeling</strong></h2>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c5337f",
   "metadata": {},
   "source": [
    "<img src=\"AS Portfolio 2 Beoordeling.png\" alt=\"AS Portfolio 2 Beoordeling\" style=\"width:600px;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
